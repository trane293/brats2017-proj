{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from modules.configfile import config\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "from scipy.ndimage.measurements import center_of_mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open mean and variance file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_var = pickle.load(open(config['saveMeanVarFilepath'] + '/HDF5_Datasetstraining_data_hgg_mean_var.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open new database with cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = h5py.File(config['hdf5_filepath_cropped'], mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_g = hdf5_file['preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mean_std(im, mean_var):\n",
    "    # expects a dictionary of means and VARIANCES, NOT STD\n",
    "    for m in range(0,4):\n",
    "        if len(np.shape(m)) > 4:\n",
    "            im[:,m,...] = (im[:,m,...] - mean_var['mn'][m]) / np.sqrt(mean_var['var'][m])\n",
    "        else:\n",
    "            im[m,...] = (im[m,...] - mean_var['mn'][m]) / np.sqrt(mean_var['var'][m])\n",
    "            \n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the HGG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_hgg = hdf5_file_g['training_data_hgg']\n",
    "training_data_segmasks_hgg = hdf5_file_g['training_data_segmasks_hgg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the Iteration here. This is the \"Epoch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random access order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(0, training_data_hgg.shape[0]))\n",
    "shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split indices into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = int((len(indices) * config['data_split']['train']) / 100.0)\n",
    "train_indices = indices[0:train_end]\n",
    "test_indices = indices[train_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the patch generation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the segmentation mask, find centroid and diameter of tumor region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in train_indices:\n",
    "    patient_x_train = apply_mean_std(training_data_hgg[idx], mean_var)\n",
    "    patient_y_train = training_data_segmasks_hgg[idx]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's call this (x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" height=\"279\" src=\"https://www.med.upenn.edu/sbia/assets/user-content/BRATS_tasks.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue). (Figure taken from the BraTS IEEE TMI paper.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, in the segmentation mask, the encoding is this - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1 for necrosis\n",
    "    2 for edema\n",
    "    3 for non-enhancing tumor\n",
    "    4 for enhancing tumor\n",
    "    0 for everything else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be able to weight the centre of mass correctly, the encoding needs to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_reweighted = np.copy(patient_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_reweighted[np.where(patient_y_train == 1)] = 10 # necrotic, the most inner region, has highest weight\n",
    "seg_reweighted[np.where(patient_y_train == 4)] = 9 # enhancing\n",
    "seg_reweighted[np.where(patient_y_train == 3)] = 8 # non-enhancing\n",
    "seg_reweighted[np.where(patient_y_train == 2)] = 7 # edema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_x, m_y, m_z = center_of_mass(seg_reweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to find the extent of mass, in all directions - (x, y, z). This is the \"standard deviation\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked this using visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid(patch_coords):\n",
    "    patch_coords = [int(x) for x in patch_coords]\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = patch_coords\n",
    "    \n",
    "    if xmin >=0 and xmax < config['size_after_cropping'][0] and \\\n",
    "                    ymin >=0 and ymax < config['size_after_cropping'][1] and \\\n",
    "                    zmin >=0 and zmax < config['size_after_cropping'][2]:\n",
    "        return patch_coords\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = np.where(patient_y_train > 0)\n",
    "std_x = np.max(x) - np.min(x)\n",
    "std_y = np.max(y) - np.min(y)\n",
    "std_z = np.max(z) - np.min(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "while k != None:\n",
    "    patch_size_x, patch_size_y, patch_size_z = (60, 60, 60)\n",
    "    std_scale = 1.8\n",
    "    xmin, ymin, zmin = np.random.multivariate_normal(mean=[m_x, m_y, m_z], cov=np.diag(np.array([std_x, std_y, std_z])*std_scale))\n",
    "    xmax = xmin + patch_size_x\n",
    "    ymax = ymin + patch_size_y\n",
    "    zmax = zmin + patch_size_z\n",
    "    patch_coords = [xmin, xmax, ymin, ymax, zmin, zmax]\n",
    "    t = check_valid(patch_coords)\n",
    "    if t != None:\n",
    "        k = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCOM_STD(segmask):\n",
    "    seg_reweighted = np.copy(segmask)\n",
    "\n",
    "    # brute force way to make sure the COM calculation is weighted correctly. We need more\n",
    "    # weight on necrotic region, than edema.\n",
    "    seg_reweighted[np.where(segmask == 1)] = 10  # necrotic, the most inner region, has highest weight\n",
    "    seg_reweighted[np.where(segmask == 4)] = 9  # enhancing\n",
    "    seg_reweighted[np.where(segmask == 3)] = 8  # non-enhancing\n",
    "    seg_reweighted[np.where(segmask == 2)] = 7  # edema\n",
    "\n",
    "    # calculate COM\n",
    "    m_x, m_y, m_z = center_of_mass(seg_reweighted)\n",
    "\n",
    "    x, y, z = np.where(segmask > 0)\n",
    "    std_x = np.max(x) - np.min(x)\n",
    "    std_y = np.max(y) - np.min(y)\n",
    "    std_z = np.max(z) - np.min(z)\n",
    "    \n",
    "    return [m_x, m_y, m_z], [std_x, std_y, std_z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIsualize the patch in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mayavi import mlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDense(bbox, im):\n",
    "    box = np.zeros(im.shape)\n",
    "    box[bbox[0]:bbox[1], bbox[2]:bbox[3], bbox[4]:bbox[5]] = 1\n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get a segmentation\n",
    "seg = patient_y_train\n",
    "\n",
    "dense_bbox = createDense(t, seg)\n",
    "\n",
    "src = mlab.pipeline.scalar_field(seg)\n",
    "\n",
    "src_bbox = mlab.pipeline.scalar_field(dense_bbox)\n",
    "# mlab.pipeline.iso_surface(src, contours=[0, 1, 2, 3, 4], opacity=0.5)\n",
    "mlab.pipeline.iso_surface(src, contours=[1], opacity=0.4, color=(0,1,0))\n",
    "mlab.pipeline.iso_surface(src, contours=[2], opacity=0.4)\n",
    "mlab.pipeline.iso_surface(src, contours=[3], opacity=0.4)\n",
    "mlab.pipeline.iso_surface(src, contours=[4], opacity=0.4)\n",
    "mlab.pipeline.iso_surface(src_bbox, contours=[1], opacity=0.2)\n",
    "# mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)\n",
    "mlab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry run the patch generation pipeline and manually see the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e315fa18343a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mmlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mmlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for idx in train_indices[3:]:\n",
    "    patient_x_train = apply_mean_std(training_data_hgg[idx], mean_var)\n",
    "    patient_y_train = training_data_segmasks_hgg[idx]\n",
    "    \n",
    "    seg_reweighted = np.copy(patient_y_train)\n",
    "    \n",
    "    seg_reweighted[np.where(patient_y_train == 1)] = 10 # necrotic, the most inner region, has highest weight\n",
    "    seg_reweighted[np.where(patient_y_train == 4)] = 9 # enhancing\n",
    "    seg_reweighted[np.where(patient_y_train == 3)] = 8 # non-enhancing\n",
    "    seg_reweighted[np.where(patient_y_train == 2)] = 7 # edema\n",
    "    \n",
    "    m_x, m_y, m_z = center_of_mass(seg_reweighted)\n",
    "    \n",
    "    for _num in range(0, 10):\n",
    "        k = 0\n",
    "        while k != None:\n",
    "            patch_size_x, patch_size_y, patch_size_z = (40, 40, 40)\n",
    "            std_scale = 400\n",
    "            xc, yc, zc = np.random.multivariate_normal(mean=[m_x, m_y, m_z], cov=np.diag(np.array([std_x, std_y, std_z])*std_scale))\n",
    "            xmin = xc - patch_size_x\n",
    "            xmax = xc + patch_size_x\n",
    "            \n",
    "            ymin = yc - patch_size_y\n",
    "            ymax = yc + patch_size_y\n",
    "            \n",
    "            zmin = zc - patch_size_z\n",
    "            zmax = zc + patch_size_z\n",
    "            \n",
    "#             xmax = xmin + patch_size_x\n",
    "#             ymax = ymin + patch_size_y\n",
    "#             zmax = zmin + patch_size_z\n",
    "            patch_coords = [xmin, xmax, ymin, ymax, zmin, zmax]\n",
    "            t = check_valid(patch_coords)\n",
    "            if t != None:\n",
    "                k = None\n",
    "\n",
    "        # lets get a segmentation\n",
    "        seg = patient_y_train\n",
    "\n",
    "        dense_bbox = createDense(t, seg)\n",
    "\n",
    "        src = mlab.pipeline.scalar_field(seg)\n",
    "\n",
    "        src_bbox = mlab.pipeline.scalar_field(dense_bbox)\n",
    "        # mlab.pipeline.iso_surface(src, contours=[0, 1, 2, 3, 4], opacity=0.5)\n",
    "        mlab.pipeline.iso_surface(src, contours=[1], opacity=0.4, color=(0,1,0))\n",
    "        mlab.pipeline.iso_surface(src, contours=[2], opacity=0.4)\n",
    "        mlab.pipeline.iso_surface(src, contours=[3], opacity=0.4)\n",
    "        mlab.pipeline.iso_surface(src, contours=[4], opacity=0.4)\n",
    "        mlab.pipeline.iso_surface(src_bbox, contours=[1], opacity=0.2)\n",
    "        # mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)\n",
    "        mlab.show()\n",
    "        count += 1\n",
    "        if count > 30:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
