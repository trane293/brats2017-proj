{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from modules.configfile import config\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open mean and variance file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_var = pickle.load(open(config['saveMeanVarCombinedData'], 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mn': [101.05125, 104.49648, 111.35362, 70.27612],\n",
       " 'var': [299264.5, 362103.22, 317921.5, 319424.38]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open new database with cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = h5py.File(config['hdf5_combined'], mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_g = hdf5_file['combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285, 4, 240, 240, 155)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf5_file_g['training_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mean_std(im, mean_var):\n",
    "    # expects a dictionary of means and VARIANCES, NOT STD\n",
    "    for m in range(0,4):\n",
    "        if len(np.shape(m)) > 4:\n",
    "            im[:,m,...] = (im[:,m,...] - mean_var['mn'][m]) / np.sqrt(mean_var['var'][m])\n",
    "        else:\n",
    "            im[m,...] = (im[m,...] - mean_var['mn'][m]) / np.sqrt(mean_var['var'][m])\n",
    "            \n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the HGG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_hgg = hdf5_file_g['training_data_hgg']\n",
    "# training_data_segmasks_hgg = hdf5_file_g['training_data_segmasks_hgg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the Iteration here. This is the \"Epoch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random access order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(0, hdf5_file_g['training_data'].shape[0]))\n",
    "shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split indices into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = int((len(indices) * config['data_split']['train']) / 100.0)\n",
    "train_indices = indices[0:train_end]\n",
    "test_indices = indices[train_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[277, 67, 207, 140, 148]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the train_seg.py, this is the original indices:\n",
    "\n",
    "[255, 219, 107, 230, 46, 165, 103, 151, 176]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255, 219, 107, 230, 46, 165, 103, 151, 176]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test set which is independent of any patient that was used in multimodal-synthesis model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multimodal-synthesis model was trained with indices 0-54 from the Combined HDF5 file. Do not use any data from this range as your test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_ind = [x for x in test_indices if x > 54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255, 219, 107, 230, 165, 103, 151, 176]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_data contains the test set which neither multimodal-synthesis model not BRATS model has seen. So it will be a fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = hdf5_file_g['training_data'][sorted(new_test_ind)]\n",
    "test_data_masks = hdf5_file_g['training_data_segmasks'][sorted(new_test_ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4, 240, 240, 155)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 240, 240, 155)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dice Coefficient in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(im1, im2, empty_score=1.0):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "\n",
    "    if im1.shape != im2.shape:\n",
    "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
    "\n",
    "    im_sum = im1.sum() + im2.sum()\n",
    "    if im_sum == 0:\n",
    "        return empty_score\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "\n",
    "    return 2. * intersection.sum() / im_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BRATS model for segmentation of the test_data patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.training_helpers import standardize\n",
    "from modules.vizhelpercode import viewArbitraryVolume\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defmodelfile = 'isensee'\n",
    "model_name = '/home/anmol/mounts/cedar-rm/scratch/asa224/model-staging/isensee_da_noanneal64--0.64.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldefmodule = importlib.import_module('defmodel.' + defmodelfile, package=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objs = modeldefmodule.custom_loss()\n",
    "model = modeldefmodule.open_model_with_hyper_and_history(name=model_name, custom_obj=custom_objs, load_model_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_shape = test_data.shape\n",
    "test_data_pred = np.zeros((td_shape[0], 3, td_shape[2], td_shape[3], td_shape[4]))\n",
    "\n",
    "for i in range(0, test_data.shape[0]):\n",
    "    print('Patient {}'.format(i+1))\n",
    "    pat_volume = test_data[i]\n",
    "    print('Standardizing..')\n",
    "    pat_volume = standardize(pat_volume, applyToTest=mean_var)\n",
    "\n",
    "    curr_shape = list(pat_volume.shape)\n",
    "    curr_shape.insert(0, 1) # insert 1 at index 0 to make reshaping easy\n",
    "\n",
    "    pat_volume = pat_volume.reshape(curr_shape)\n",
    "\n",
    "    # SUPER HACK WAY TO CHANGE VOLUME COMPATIBILITY WITH ISENSEE MODEL. MAKE 155 = 160\n",
    "    new_pat_volume = np.zeros((1, 4, 240, 240, 160))\n",
    "    new_pat_volume[:, :, :, :, 0:155] = pat_volume\n",
    "\n",
    "    print('Starting prediction..')\n",
    "    # predict using the whole volume\n",
    "    pred = model.predict(new_pat_volume)\n",
    "\n",
    "    # get back the main volume and strip the padding\n",
    "    pred = pred[:,:,:,:,0:155]\n",
    "\n",
    "    assert pred.shape == (1,3,240,240,155)\n",
    "\n",
    "    print('Adding predicted volume to test_data_pred store..')\n",
    "    # we use the batch size = 1 for prediction, so the first one.\n",
    "    test_data_pred[i] = pred[0]\n",
    "    viewArbitraryVolume(test_data_pred[i], slice_idx=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save test_data_pred numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load multimodal-synthesis model trained with T1-T2 as input and T2FLAIR as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"theano\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "../multimodal/model.py:5: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/home/anmol/.virtualenvs/brats/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 1064, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2724, in run_cell\n",
      "    self.events.trigger('post_run_cell')\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/IPython/core/events.py\", line 74, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/ipykernel/pylab/backend_inline.py\", line 160, in configure_once\n",
      "    activate_matplotlib(backend)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/IPython/core/pylabtools.py\", line 315, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/matplotlib/__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/home/anmol/.virtualenvs/brats/local/lib/python2.7/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "from multimodal.loader_multimodal import Data\n",
    "from multimodal.runner import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIX THIS: The data loading has to be handled in a way that multimodal-synthesis does, in the form of slices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T1\n",
      "Loading T2\n",
      "Loading T2FLAIR\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/anmol/mounts/cedar-rm/rrg_proj_dir/multimodal_brain_synthesis/RESULTS/split57_no_ST_LGG_only_T1_T2-T2FLAIR/'\n",
    "data_dir = '/home/anmol/projects/multimodal_brain_synthesis/npz_BRATS/'\n",
    "data = Data(data_dir, dataset='BRATS', trim_and_downsample=False, modalities_to_load=['T1', 'T2', 'T2FLAIR'],\n",
    "            normalize_volumes=False)\n",
    "data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 240, 240)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.T1[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_modalities = ['T1', 'T2']\n",
    "output_weights = {'T2FLAIR': 1.0, 'concat': 1.0}\n",
    "print('Creating experiment...')\n",
    "exp = Experiment(input_modalities, output_weights, './RESULTS', data=None, latent_dim=16, spatial_transformer=False)\n",
    "\n",
    "load_input_modalities = ['T1', 'T2']\n",
    "load_output_modalities = 'T2FLAIR'\n",
    "\n",
    "print('Loading partial model')\n",
    "exp.load_partial_model(folder=model_path, model_name='model', input_modalities=load_input_modalities,\n",
    "                       output_modality=load_output_modalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare T1 and T2 data for the synthesis model <br>\n",
    "```\n",
    "if 't1.' \n",
    "    i = 0\n",
    "    seq_name = 't1'\n",
    "elif 't2.' in imagefile:\n",
    "    i = 1\n",
    "    seq_name = 't2'\n",
    "elif 't1ce.' in imagefile:\n",
    "    i = 2\n",
    "    seq_name = 't1ce'\n",
    "elif 'flair.' in imagefile:\n",
    "    i = 3\n",
    "    seq_name = 'flair'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = test_data[:,0,]\n",
    "t2 = test_data[:,1,]\n",
    "\n",
    "test_data_list = [t1, t2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, 240, 240)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start prediction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = exp.mm.model.predict(test_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the patch generation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the segmentation mask, find centroid and diameter of tumor region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in train_indices:\n",
    "    patient_x_train = apply_mean_std(training_data_hgg[idx], mean_var)\n",
    "    patient_y_train = training_data_segmasks_hgg[idx]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's call this (x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" height=\"279\" src=\"https://www.med.upenn.edu/sbia/assets/user-content/BRATS_tasks.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue). (Figure taken from the BraTS IEEE TMI paper.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, in the segmentation mask, the encoding is this - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1 for necrosis\n",
    "    2 for edema\n",
    "    3 for non-enhancing tumor\n",
    "    4 for enhancing tumor\n",
    "    0 for everything else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be able to weight the centre of mass correctly, the encoding needs to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_reweighted = np.copy(patient_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_reweighted[np.where(patient_y_train == 1)] = 10 # necrotic, the most inner region, has highest weight\n",
    "seg_reweighted[np.where(patient_y_train == 4)] = 9 # enhancing\n",
    "seg_reweighted[np.where(patient_y_train == 3)] = 8 # non-enhancing\n",
    "seg_reweighted[np.where(patient_y_train == 2)] = 7 # edema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_x, m_y, m_z = center_of_mass(seg_reweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to find the extent of mass, in all directions - (x, y, z). This is the \"standard deviation\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked this using visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid(patch_coords):\n",
    "    patch_coords = [int(x) for x in patch_coords]\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = patch_coords\n",
    "    \n",
    "    if xmin >=0 and xmax < config['size_after_cropping'][0] and \\\n",
    "                    ymin >=0 and ymax < config['size_after_cropping'][1] and \\\n",
    "                    zmin >=0 and zmax < config['size_after_cropping'][2]:\n",
    "        return patch_coords\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = np.where(patient_y_train > 0)\n",
    "std_x = np.max(x) - np.min(x)\n",
    "std_y = np.max(y) - np.min(y)\n",
    "std_z = np.max(z) - np.min(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "while k != None:\n",
    "    patch_size_x, patch_size_y, patch_size_z = (60, 60, 60)\n",
    "    std_scale = 1.8\n",
    "    xmin, ymin, zmin = np.random.multivariate_normal(mean=[m_x, m_y, m_z], cov=np.diag(np.array([std_x, std_y, std_z])*std_scale))\n",
    "    xmax = xmin + patch_size_x\n",
    "    ymax = ymin + patch_size_y\n",
    "    zmax = zmin + patch_size_z\n",
    "    patch_coords = [xmin, xmax, ymin, ymax, zmin, zmax]\n",
    "    t = check_valid(patch_coords)\n",
    "    if t != None:\n",
    "        k = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCOM_STD(segmask):\n",
    "    seg_reweighted = np.copy(segmask)\n",
    "\n",
    "    # brute force way to make sure the COM calculation is weighted correctly. We need more\n",
    "    # weight on necrotic region, than edema.\n",
    "    seg_reweighted[np.where(segmask == 1)] = 10  # necrotic, the most inner region, has highest weight\n",
    "    seg_reweighted[np.where(segmask == 4)] = 9  # enhancing\n",
    "    seg_reweighted[np.where(segmask == 3)] = 8  # non-enhancing\n",
    "    seg_reweighted[np.where(segmask == 2)] = 7  # edema\n",
    "\n",
    "    # calculate COM\n",
    "    m_x, m_y, m_z = center_of_mass(seg_reweighted)\n",
    "\n",
    "    x, y, z = np.where(segmask > 0)\n",
    "    std_x = np.max(x) - np.min(x)\n",
    "    std_y = np.max(y) - np.min(y)\n",
    "    std_z = np.max(z) - np.min(z)\n",
    "    \n",
    "    return [m_x, m_y, m_z], [std_x, std_y, std_z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIsualize the patch in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mayavi import mlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDense(bbox, im):\n",
    "    box = np.zeros(im.shape)\n",
    "    box[bbox[0]:bbox[1], bbox[2]:bbox[3], bbox[4]:bbox[5]] = 1\n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get a segmentation\n",
    "seg = patient_y_train\n",
    "\n",
    "dense_bbox = createDense(t, seg)\n",
    "\n",
    "src = mlab.pipeline.scalar_field(seg)\n",
    "\n",
    "src_bbox = mlab.pipeline.scalar_field(dense_bbox)\n",
    "# mlab.pipeline.iso_surface(src, contours=[0, 1, 2, 3, 4], opacity=0.5)\n",
    "mlab.pipeline.iso_surface(src, contours=[1], opacity=0.4, color=(0,1,0))\n",
    "mlab.pipeline.iso_surface(src, contours=[2], opacity=0.4)\n",
    "mlab.pipeline.iso_surface(src, contours=[3], opacity=0.4)\n",
    "mlab.pipeline.iso_surface(src, contours=[4], opacity=0.4)\n",
    "mlab.pipeline.iso_surface(src_bbox, contours=[1], opacity=0.2)\n",
    "# mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)\n",
    "mlab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry run the patch generation pipeline and manually see the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for idx in train_indices[3:]:\n",
    "    patient_x_train = apply_mean_std(training_data_hgg[idx], mean_var)\n",
    "    patient_y_train = training_data_segmasks_hgg[idx]\n",
    "    \n",
    "    seg_reweighted = np.copy(patient_y_train)\n",
    "    \n",
    "    seg_reweighted[np.where(patient_y_train == 1)] = 10 # necrotic, the most inner region, has highest weight\n",
    "    seg_reweighted[np.where(patient_y_train == 4)] = 9 # enhancing\n",
    "    seg_reweighted[np.where(patient_y_train == 3)] = 8 # non-enhancing\n",
    "    seg_reweighted[np.where(patient_y_train == 2)] = 7 # edema\n",
    "    \n",
    "    m_x, m_y, m_z = center_of_mass(seg_reweighted)\n",
    "    \n",
    "    for _num in range(0, 10):\n",
    "        k = 0\n",
    "        while k != None:\n",
    "            patch_size_x, patch_size_y, patch_size_z = (40, 40, 40)\n",
    "            std_scale = 400\n",
    "            xc, yc, zc = np.random.multivariate_normal(mean=[m_x, m_y, m_z], cov=np.diag(np.array([std_x, std_y, std_z])*std_scale))\n",
    "            xmin = xc - patch_size_x\n",
    "            xmax = xc + patch_size_x\n",
    "            \n",
    "            ymin = yc - patch_size_y\n",
    "            ymax = yc + patch_size_y\n",
    "            \n",
    "            zmin = zc - patch_size_z\n",
    "            zmax = zc + patch_size_z\n",
    "            \n",
    "#             xmax = xmin + patch_size_x\n",
    "#             ymax = ymin + patch_size_y\n",
    "#             zmax = zmin + patch_size_z\n",
    "            patch_coords = [xmin, xmax, ymin, ymax, zmin, zmax]\n",
    "            t = check_valid(patch_coords)\n",
    "            if t != None:\n",
    "                k = None\n",
    "\n",
    "        # lets get a segmentation\n",
    "        seg = patient_y_train\n",
    "\n",
    "        dense_bbox = createDense(t, seg)\n",
    "\n",
    "        src = mlab.pipeline.scalar_field(seg)\n",
    "\n",
    "        src_bbox = mlab.pipeline.scalar_field(dense_bbox)\n",
    "        # mlab.pipeline.iso_surface(src, contours=[0, 1, 2, 3, 4], opacity=0.5)\n",
    "        mlab.pipeline.iso_surface(src, contours=[1], opacity=0.4, color=(0,1,0))\n",
    "        mlab.pipeline.iso_surface(src, contours=[2], opacity=0.4)\n",
    "        mlab.pipeline.iso_surface(src, contours=[3], opacity=0.4)\n",
    "        mlab.pipeline.iso_surface(src, contours=[4], opacity=0.4)\n",
    "        mlab.pipeline.iso_surface(src_bbox, contours=[1], opacity=0.2)\n",
    "        # mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)\n",
    "        mlab.show()\n",
    "        count += 1\n",
    "        if count > 30:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
